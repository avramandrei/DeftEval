# DeftEval

This repository contains the source code for the first and the second task of [DeftEval 2020 competition](https://competitions.codalab.org/competitions/22759)
submitted by the team at University Politehnica of Bucharest (UPB).

We fined-tuned frozen and non-frozen Transformer-based language models using the [HuggingFace](https://github.com/huggingface/transformers) framework, together with a multi-task model that jointly predicts the outputs for all the third tasks. 

The code for each task, with additional details on how to use it, can be found in **task1** and **task2** directories.

## Cite
Please consider citing the following [paper](https://arxiv.org/abs/2009.05603) as a thank you to the authors: 
```
@article{avram2020upb,
  title={UPB at SemEval-2020 Task 6: Pretrained Language Models for DefinitionExtraction},
  author={Avram, Andrei-Marius and Cercel, Dumitru-Clementin and Chiru, Costin-Gabriel},
  journal={arXiv preprint arXiv:2009.05603},
  year={2020}
}
```
